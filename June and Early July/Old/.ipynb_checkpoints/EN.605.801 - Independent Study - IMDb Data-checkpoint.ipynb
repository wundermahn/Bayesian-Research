{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EN.605.801 \n",
    "# Independent Study in Computer Science\n",
    "\n",
    "## Bayesian Approach to Sentiment Analysis Classification\n",
    "\n",
    "This notebook intends to provide project status through 06/18/2020. \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### The accomplished action items thus far:\n",
    "\n",
    " - Scale all positive sentiment values x 3 while keeping negative sentiment values at x1\n",
    " - Implement K-CFV scheme into testing\n",
    " - Create framework for monte-carlo simulations of model performance\n",
    " - Create visuals for model performance\n",
    " - POS Tagging\n",
    " - Curve fitting documentation complete\n",
    " - Git repository updated\n",
    " - Extended to multiclass Amazon review dataset\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### The following action items require additional work:\n",
    " - Determine feature importances\n",
    " - Code clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources Used (as of 06/25/2020)\n",
    "&nbsp;\n",
    "\n",
    "## Text Parsing\n",
    " - https://github.com/wundermahn/Yelp-Classification-ML\n",
    " \n",
    " &nbsp;\n",
    " \n",
    "## NLP Resources\n",
    " - https://nlp.stanford.edu/IR-book/information-retrieval-book.html\n",
    " - https://nlp.stanford.edu/\n",
    " - https://www.nltk.org/\n",
    " - https://www.nltk.org/book/ch05.html\n",
    " \n",
    " &nbsp;\n",
    " \n",
    "## ML / Classification Resources\n",
    " - https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    " - https://scikit-learn.org/stable/modules/cross_validation.html\n",
    " - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    " - https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection\n",
    " \n",
    " &nbsp;\n",
    " \n",
    "## Academic Whitepapers\n",
    " - https://www.researchgate.net/publication/221650814_Spam_Filtering_with_Naive_Bayes_-_Which_Naive_Bayes\n",
    " - https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf (**)\n",
    " - https://www.researchgate.net/publication/221439320_The_Optimality_of_Naive_Bayes\n",
    " - https://www.cs.cmu.edu/~knigam/papers/multinomial-aaaiws98.pdf\n",
    " - https://www.sciencedirect.com/science/article/pii/S2090447914000550\n",
    " - https://www.sciencedirect.com/science/article/pii/S0888613X08001400\n",
    " \n",
    " &nbsp;\n",
    " \n",
    "## Data\n",
    " - https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "     - https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf\n",
    " - https://analyticsindiamag.com/10-popular-datasets-for-sentiment-analysis/\n",
    "     - http://www.cs.jhu.edu/~mdredze/datasets/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform all necessary imports\n",
    "\n",
    "import pandas as pd, os, gc as gc, nltk, re, string, numpy as np, time, pickle, warnings\n",
    "import matplotlib.pyplot as plt, plotly.express as px\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, ShuffleSplit, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.svm import SVC # Per industry \"best practice\"\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the distributions of each word in the df\n",
    "def plot_df_distributions(df, name):\n",
    "    # Loop through the df\n",
    "    for col in df.columns:\n",
    "        # Create a temp numpy array\n",
    "        temp = df[col].to_numpy()\n",
    "        # Plot the distribution\n",
    "        n, bins, patches = plt.hist(temp, bins=10)\n",
    "        # Set the title\n",
    "        plt.suptitle(\"Data Type: {} | {} Distribution\".format(str(name), str(col)))\n",
    "        # Show the figure\n",
    "        plt.show()\n",
    "        \n",
    "# Function to plot the distributions of each word in the df\n",
    "def plot_pos_distributions(df, cols):\n",
    "    # Loop through the df\n",
    "    for col in cols:\n",
    "        # Create a temp numpy array\n",
    "        temp = df[col].to_numpy()\n",
    "        # Plot the distribution\n",
    "        n, bins, patches = plt.hist(temp, bins=10)\n",
    "        # Set the title\n",
    "        plt.suptitle(\"POS Tag: {} | Distribution\".format(str(col)))\n",
    "        # Show the figure\n",
    "        plt.show()        \n",
    "\n",
    "# Function to plot the results of the classifiers\n",
    "def plot_results(df, name, metric):\n",
    "    # Create a plotly figure, and plot whichever metric (best, worst, average) is passed\n",
    "    fig = px.line(df, x='Simulation', y=metric, color='Classifier')\n",
    "    # Update title/labels\n",
    "    fig.update_layout(title='{} Data Classifier Performance'.format(name), xaxis_title='Simulation',\n",
    "                             yaxis_title='Accuracy')\n",
    "    # Display the figure\n",
    "    fig.show()     \n",
    "    \n",
    "# This function removes numbers from an array\n",
    "def remove_nums(arr): \n",
    "    # Declare a regular expression\n",
    "    pattern = '[0-9]'  \n",
    "    # Remove the pattern, which is a number\n",
    "    arr = [re.sub(pattern, '', i) for i in arr]    \n",
    "    # Return the array with numbers removed\n",
    "    return arr\n",
    "\n",
    "# This function cleans the passed in paragraph and parses it\n",
    "def get_words(para, stem):   \n",
    "    # Create a set of stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Split it into lower case    \n",
    "    lower = para.lower().split()\n",
    "    # Remove punctuation\n",
    "    no_punctuation = (nopunc.translate(str.maketrans('', '', string.punctuation)) for nopunc in lower)\n",
    "    # Remove integers\n",
    "    no_integers = remove_nums(no_punctuation)\n",
    "    # Remove stop words\n",
    "    dirty_tokens = (data for data in no_integers if data not in stop_words)\n",
    "    # Ensure it is not empty\n",
    "    tokens = [data for data in dirty_tokens if data.strip()]\n",
    "    # Ensure there is more than 1 character to make up the word\n",
    "    tokens = [data for data in tokens if len(data) > 1]\n",
    "       \n",
    "    if stem == True:\n",
    "        # Perform stemming\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "        return stemmed_tokens\n",
    "    \n",
    "    else:\n",
    "        # Return the tokens\n",
    "        return tokens \n",
    "\n",
    "# This function parses NLTK returned POS tuples\n",
    "def parse_tuples(list_of_tuples, verbose):\n",
    "    \n",
    "    # Declare POS counts\n",
    "    cnt_noun = 0\n",
    "    cnt_adj = 0\n",
    "    cnt_vb = 0\n",
    "    cnt_other = 0\n",
    "    \n",
    "    # Loop through the returned tuples\n",
    "    for tpl in list_of_tuples:\n",
    "        \n",
    "        # NOTE - If needed, verbose printing is available to\n",
    "        # check for completeness.\n",
    "        \n",
    "        # If the word is a noun, increase the noun count\n",
    "        if('NN' in tpl[1]):\n",
    "            cnt_noun += 1\n",
    "            if(verbose):\n",
    "                print(\"Noun: {}\".format(tpl))\n",
    "        # If the word is an adjective, increase the adjective count\n",
    "        elif('JJ' in tpl[1]):\n",
    "            cnt_adj += 1\n",
    "            if(verbose):\n",
    "                print(\"Adjective: {}\".format(tpl))\n",
    "        # If the word is a verb, increase the verb count\n",
    "        elif('VB' in tpl[1] or 'VP' in tpl[1]):\n",
    "            cnt_vb += 1\n",
    "            if(verbose):\n",
    "                print(\"Verb: {}\".format(tpl))\n",
    "        # If the word isn't one of those 3, increase the other count\n",
    "        else:\n",
    "            cnt_other += 1\n",
    "            if(verbose):\n",
    "                print(\"Other: {}\".format(tpl))\n",
    "    \n",
    "    # Return the counts\n",
    "    return cnt_noun, cnt_adj, cnt_vb, cnt_other    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Using K-CFV and Simulated Results\n",
    "This section will move to using 5-Fold CFV and to simulate results over multiple trials vs 1 static. Both the fold count and trial count are variables set below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 5\n",
    "sims = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data built by Iteration 1\n",
    "boolean = pd.read_csv('Trimmed_Boolean.csv')\n",
    "tfidf = pd.read_csv('Trimmed_TFIDF.csv')\n",
    "freq = pd.read_csv('Trimmed_Count.csv')\n",
    "\n",
    "# Create lists of the dataframes, and their names\n",
    "dfs = [boolean, tfidf, freq]\n",
    "names = ['Boolean', 'TFIDF', 'Frequency']\n",
    "\n",
    "# Drop weird column pandas creates when writing to csv\n",
    "for df in dfs:\n",
    "    df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start calculating their correlation\n",
    "\n",
    "## TODO: Turn this into a function\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    # Set figure and axis size\n",
    "    fig, ax = plt.subplots(figsize = (15,12))\n",
    "    # Create a temp copy of the df\n",
    "    temp = dfs[i].copy()\n",
    "    # https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on\n",
    "    # Create a correlation matrix\n",
    "    corr_matrix = temp.corr(method='spearman').abs()\n",
    "    \n",
    "    # Do some cleaning to remove features that have more than a 95% correlation, or a less trhan 1% correlation\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95) or any(upper[column] < 0.01)]\n",
    "    temp.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    # Now, re calculate the correlation with good columns\n",
    "    corr = temp.corr(method='spearman')\n",
    "    \n",
    "    # Create a heatmap\n",
    "    _ = sns.heatmap(corr, fmt=\"f\", linewidths=0.25, center=0, cmap='coolwarm', linecolor='black')\n",
    "    # Set the title\n",
    "    ax.set_title('{} Correlation Heatmap'.format(names[i]))\n",
    "    \n",
    "    # Show the plot, then close it\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get some quick plots to show the distributions of each word in each dataset\n",
    "for i in range(len(dfs)):\n",
    "    plot_df_distributions(dfs[i], names[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test some basic classifiers\n",
    "\n",
    "# Declare a list of classifiers to try\n",
    "# classifiers = [GaussianNB(), MultinomialNB(), BernoulliNB(), \n",
    "#                SVC(C=1.0, kernel='linear', degree=3, gamma='auto')]\n",
    "\n",
    "classifiers = [GaussianNB(), MultinomialNB(), BernoulliNB()]\n",
    "\n",
    "# Create blank dataframes to hold the results of the classification attempts\n",
    "boolean_results = pd.DataFrame(columns = ['Simulation', 'Classifier', 'Best', 'Worst', 'Average'])\n",
    "tfidf_results = pd.DataFrame(columns = ['Simulation', 'Classifier', 'Best', 'Worst', 'Average'])\n",
    "freq_results = pd.DataFrame(columns = ['Simulation', 'Classifier', 'Best', 'Worst', 'Average'])\n",
    "\n",
    "# Run a given number of simulations\n",
    "for n in range(sims):\n",
    "    # Loop through each of the datasets\n",
    "    for i in range(len(dfs)):\n",
    "        print(\"*--------------------------------------------*\")\n",
    "        # Copy the dataset\n",
    "        temp = dfs[i].copy()\n",
    "        # Randomly shuffle it\n",
    "        temp = temp.sample(frac=1).reset_index(drop=True)\n",
    "        # Get your classes, maintaining the same index as the data\n",
    "        classes = temp['classification']\n",
    "        temp.drop('classification', inplace=True, axis=1)\n",
    "\n",
    "        # Try each classifier\n",
    "        for clf in classifiers:\n",
    "            # Take the classifiers name\n",
    "            name = str(clf.__class__.__name__)\n",
    "            try:  \n",
    "                # Utilize a shuffle split for the cross validation\n",
    "                cv = ShuffleSplit(n_splits = folds, test_size = 0.33, random_state=np.random.randint(1,100))\n",
    "                # Capture the best, worst, and average across the splits\n",
    "                best = round(max(cross_val_score(clf, temp, classes, cv=cv)), 5)\n",
    "                worst = round(min(cross_val_score(clf, temp, classes, cv=cv)), 5)\n",
    "                avg = round((sum(cross_val_score(clf, temp, classes, cv=cv)) / len(cross_val_score(clf, temp, classes, cv=cv))), 5)\n",
    "                # Print out results\n",
    "                print(\"Simulation: {} | Data: {} | Classifier: {} | Best: {} | Worst: {} | Average: {}\".format(n+1, names[i], name, best, worst, avg))\n",
    "            except:\n",
    "                # Alert if any error\n",
    "                print(\"Error Calculating: {}\".format(name))\n",
    "\n",
    "            # Create a row to append to the respective df with the results\n",
    "            temp_row = [n+1, name, best, worst, avg]\n",
    "            \n",
    "            # Append to correct df\n",
    "            if(names[i] == 'Boolean'):\n",
    "                boolean_results.loc[len(boolean_results), :] = temp_row\n",
    "            elif(names[i] == 'TFIDF'):\n",
    "                tfidf_results.loc[len(tfidf_results), :] = temp_row\n",
    "            else:\n",
    "                freq_results.loc[len(freq_results), :] = temp_row\n",
    "        \n",
    "        # Delete temp df\n",
    "        del(temp)\n",
    "        \n",
    "        # Free up memory\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Store all results in one list\n",
    "results = [boolean_results, tfidf_results, freq_results]   \n",
    "\n",
    "# Plot the average across the folds for each dataset\n",
    "for i in range(len(results)):\n",
    "    plot_results(results[i], names[i], 'Average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Rerunning above with scaled positive sentiment data\n",
    "This portion of the notebook analyzes the effects of multiplying all positive sentiments by 3, while leaving negative sentiments at face value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the 3x scaling suggested by Professor Johnson\n",
    "\n",
    "# First, create copies of the dfs\n",
    "scaled_boolean = boolean.copy()\n",
    "scaled_tfidf = tfidf.copy()\n",
    "scaled_freq = freq.copy()\n",
    "\n",
    "# Create list of scaled dfs\n",
    "scaled_dfs = [scaled_boolean, scaled_tfidf, scaled_freq]\n",
    "\n",
    "# Free memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per Professor Johnson's instructions\n",
    "for df in scaled_dfs:\n",
    "    df['classification'].replace(1, 3, inplace=True)\n",
    "    df['classification'].replace(-1, 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep count of a particular word, to ensure it changes\n",
    "old_written = scaled_freq['written']\n",
    "\n",
    "# Loop through the scaled dfs\n",
    "for df in scaled_dfs:\n",
    "    # Multiply the values by *-1 if the sentiment was negative\n",
    "    df.update(df.drop('classification',axis=1).mul(df.classification,axis=0)[df.classification.eq(3)])\n",
    "    # Free up some memory\n",
    "    gc.collect()\n",
    "    \n",
    "# Collect new written values\n",
    "new_written = scaled_freq['written']\n",
    "\n",
    "# Print out proof the word counts change\n",
    "print(\"Old Written Values: {} | New Written Values: {}\".format(sum(old_written), sum(new_written)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some quick plots to show the distributions of each word in each dataset\n",
    "for i in range(len(scaled_dfs)):\n",
    "    plot_df_distributions(scaled_dfs[i], names[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test some basic classifiers\n",
    "\n",
    "# Declare a list of classifiers to try\n",
    "# Declare a list of classifiers to try\n",
    "# classifiers = [GaussianNB(), MultinomialNB(), BernoulliNB(), \n",
    "#                SVC(C=1.0, kernel='linear', degree=3, gamma='auto')]\n",
    "\n",
    "classifiers = [GaussianNB(), MultinomialNB(), BernoulliNB()]\n",
    "\n",
    "# Create blank dataframes to hold the results of the classification attempts\n",
    "scaled_boolean_results = pd.DataFrame(columns = ['Simulation', 'Classifier', 'Best', 'Worst', 'Average'])\n",
    "scaled_tfidf_results = pd.DataFrame(columns = ['Simulation', 'Classifier', 'Best', 'Worst', 'Average'])\n",
    "scaled_freq_results = pd.DataFrame(columns = ['Simulation', 'Classifier', 'Best', 'Worst', 'Average'])\n",
    "\n",
    "# Run a given number of simulations\n",
    "for n in range(sims):\n",
    "    # Loop through each of the datasets\n",
    "    for i in range(len(scaled_dfs)):\n",
    "        print(\"*--------------------------------------------*\")\n",
    "        # Copy the dataset\n",
    "        temp = scaled_dfs[i].copy()\n",
    "        # Randomly shuffle it\n",
    "        temp = temp.sample(frac=1).reset_index(drop=True)\n",
    "        # Get your classes, maintaining the same index as the data\n",
    "        classes = temp['classification']\n",
    "        temp.drop('classification', inplace=True, axis=1)\n",
    "\n",
    "        # Try each classifier\n",
    "        for clf in classifiers:\n",
    "            # Take the classifiers name\n",
    "            name = str(clf.__class__.__name__)\n",
    "            try:  \n",
    "                # Utilize a shuffle split for the cross validation\n",
    "                cv = ShuffleSplit(n_splits = folds, test_size = 0.33, random_state=np.random.randint(1,100))\n",
    "                # Capture the best, worst, and average across the splits\n",
    "                best = round(max(cross_val_score(clf, temp, classes, cv=cv)), 5)\n",
    "                worst = round(min(cross_val_score(clf, temp, classes, cv=cv)), 5)\n",
    "                avg = round((sum(cross_val_score(clf, temp, classes, cv=cv)) / len(cross_val_score(clf, temp, classes, cv=cv))), 5)\n",
    "                # Print out results\n",
    "                print(\"Simulation: {} | Data: {} | Classifier: {} | Best: {} | Worst: {} | Average: {}\".format(n+1, names[i], name, best, worst, avg))\n",
    "            except:\n",
    "                # Alert if any error\n",
    "                print(\"Error Calculating: {}\".format(name))\n",
    "\n",
    "            # Create a row to append to the respective df with the results\n",
    "            temp_row = [n+1, name, best, worst, avg]\n",
    "            \n",
    "            # Append to correct df\n",
    "            if(names[i] == 'Boolean'):\n",
    "                scaled_boolean_results.loc[len(scaled_boolean_results), :] = temp_row\n",
    "            elif(names[i] == 'TFIDF'):\n",
    "                scaled_tfidf_results.loc[len(scaled_tfidf_results), :] = temp_row\n",
    "            else:\n",
    "                scaled_freq_results.loc[len(scaled_freq_results), :] = temp_row\n",
    "        \n",
    "        # Delete temp df\n",
    "        del(temp)\n",
    "        \n",
    "        # Free up memory\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store all scaled results in one list\n",
    "scaled_results = [scaled_boolean_results, scaled_tfidf_results, scaled_freq_results]   \n",
    "\n",
    "# Display their average performance across the folds\n",
    "for i in range(len(scaled_results)):\n",
    "    plot_results(scaled_results[i], names[i], 'Average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: POS Tagging\n",
    "This section will attempt to build a dataframe that, for each review, will count the number of nouns, verbs, adjectives, and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to properly POS Tag, the raw text, and not vectorized text is needed\n",
    "raw_text = pd.read_csv('complete_movie_data.csv')\n",
    "try:\n",
    "    raw_text.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "# Print out the first 10 rows\n",
    "print(raw_text.head(n=10))\n",
    "\n",
    "# Free up some memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a blank datafame\n",
    "new_df = pd.DataFrame(columns = ['ReviewID', 'Nouns', 'Verbs', 'Adjectives', 'Other', 'Class'])\n",
    "\n",
    "# Loop through the input data\n",
    "for index, row in raw_text.iterrows():\n",
    "    # Get the tokens according to discussed rules\n",
    "    tokens = get_words(row['Text'], True)\n",
    "    # Create the POS for each word\n",
    "    res = nltk.pos_tag(tokens)\n",
    "    # Parse the tuples, and get the counts\n",
    "    nouns, adjectives, verbs, other = parse_tuples(res, False)\n",
    "    # Create a temp row\n",
    "    temp_row = [row['ID'], nouns, verbs, adjectives, other, row['Class']]\n",
    "    # Append it to the dataframe\n",
    "    new_df.loc[len(new_df), :] = temp_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of word types across all documents\n",
    "plot_pos_distributions(new_df, ['Nouns', 'Verbs', 'Adjectives', 'Other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test some basic classifiers\n",
    "\n",
    "# Declare a list of classifiers to try\n",
    "# Declare a list of classifiers to try\n",
    "# classifiers = [GaussianNB(), MultinomialNB(), BernoulliNB(), \n",
    "#                SVC(C=1.0, kernel='linear', degree=3, gamma='auto')]\n",
    "\n",
    "classifiers = [GaussianNB(), MultinomialNB(), BernoulliNB()]\n",
    "\n",
    "# Create blank dataframes to hold the results of the classification attempts\n",
    "pos_results = pd.DataFrame(columns = ['Simulation', 'Classifier', 'Best', 'Worst', 'Average'])\n",
    "\n",
    "# Run a given number of simulations\n",
    "for n in range(sims):\n",
    "    print(\"*--------------------------------------------*\")\n",
    "    # Create a temp for each simulation\n",
    "    temp = new_df.copy()\n",
    "    # Randomly shuffle it\n",
    "    temp = temp.sample(frac=1).reset_index(drop=True)\n",
    "    # Get your classes, maintaining the same index as the data\n",
    "    classes = temp['Class']\n",
    "    classes = classes.astype('int')\n",
    "    # Drop irrelevant columns\n",
    "    temp.drop(['Class', 'ReviewID'], inplace=True, axis=1)\n",
    "\n",
    "    # Try each classifier\n",
    "    for clf in classifiers:\n",
    "        # Take the classifiers name\n",
    "        name = str(clf.__class__.__name__)\n",
    "        try:  \n",
    "            # Utilize a shuffle split for the cross validation\n",
    "            cv = ShuffleSplit(n_splits = folds, test_size = 0.33, random_state=np.random.randint(1,100))\n",
    "            # Capture the best, worst, and average across the splits\n",
    "            best = round(max(cross_val_score(clf, temp, classes, cv=cv)), 5)\n",
    "            worst = round(min(cross_val_score(clf, temp, classes, cv=cv)), 5)\n",
    "            avg = round((sum(cross_val_score(clf, temp, classes, cv=cv)) / len(cross_val_score(clf, temp, classes, cv=cv))), 5)\n",
    "            # Print out results\n",
    "            print(\"Simulation: {} | Data: {} | Classifier: {} | Best: {} | Worst: {} | Average: {}\".format(n+1, names[i], name, best, worst, avg))\n",
    "        except:\n",
    "            # Alert if any error\n",
    "            print(\"Error Calculating: {}\".format(name))\n",
    "\n",
    "        # Create a row to append to the respective df with the results\n",
    "        temp_row = [n+1, name, best, worst, avg]\n",
    "        pos_results.loc[len(pos_results), :] = temp_row\n",
    "        \n",
    "        # Free up memory\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all scaled results in one list\n",
    "plot_results(pos_results, \"POS Data\", 'Average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
